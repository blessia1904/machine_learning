---
title: "452 HW 3"
author: "Huixin Blessia Li"
date: "`r Sys.Date()`"
output: html_document
---

# Question 1  
## a. How would you choose a sample subset (such as missing value, nulls, empty columns) of this dataset? What criteria would you consider when selecting a training subset from the above dataset (such as balanced distribution between training and test for the treated observations) ?  
Empty columns, nulls, and missing values should be excluded from the dataset as they do not contribute to prediction; thus, columns "family record," "past record," and "wrist dim"(the one only has two values) were removed, so were the missing values. Since missing values and nulls were removed, training subset can be selected by randomized splitting. Balanced or proportional distribution between training and test might be needed if we treat missing values (like replace them with mean or mode). 
```{r}
heart <- read.csv("/Users/HuixinLi/Desktop/452\ ML/HW\ 3/heart.csv")
str(heart)
heart <- heart[-c(7,11,15)] ## remove 3 empty columns

sum(is.na(heart)) ## 10 missing values
heart_clean <- na.omit(heart) ## remove missing values
#View(heart_clean)
summary(heart_clean)
```

## b. Randomly split the dataset into test and training sets using 80% observations as training set. Fit a simple linear regression model (full model) to predict the heart attack probability and test your model against the test set.  Explain your model and obtain the R^2 for the predictions of the test data set (i.e., a true OOS R^2).
```{r}
library(caret)
set.seed(5)

#Use 80% of dataset as training set and remaining 20% as testing set
sample_size <- floor(0.8 * nrow(heart_clean))
sample <- sample(nrow(heart_clean), size = sample_size)

train <- heart_clean[sample,]
test <- heart_clean[-sample,]

#view dimensions of training and test set
dim(train)
dim(test)

#Fit a simple linear regression model - full model
train_model_glm <- glm(formula = heart_attack ~., data=train)
summary(train_model_glm)
```

```{r}
#test model against the test set
test_x <- test[,1:16]
test_y <- test$heart_attack

test_y_pred <- predict(train_model_glm, newdata=test_x)
test_y_mean <- mean(test_y)
#obtain OOS R^2
dev <- sum((test_y - test_y_pred)^2)
null_dev <- sum((test_y - test_y_mean)^2)

(OOS_R <- 1-(dev/null_dev))

```
Based on the model results, we are 95% confident that weight, neck_dim, chest dim, abdom dim, thigh_dim, ankle_dim, and biceps_dim are positively related to the probability of heart attack, which make sense according to general health knowledge. We are also 95% confident to say that height, fat-free weight, and knee_dim are negatively related to the probability of having heart attack.  
The out of sample R-squared of the model is 87.45%, meaning the model's out of sample prediction power is great.

# Question 2  
## Explain cross-validation and highlight the problems that may be associated with a cross-validation approach.
Cross validation is to assess how well a model can perform on a new/independent data set through iterative re-sampling with different subsets of the data set that are used to train and test a model. In class we learned the k-fold method, which means we repeat the re-sample process k times.

A potential problem associated with this approach is that in the process of cross-validation, there are some subsets stay across different rounds of re-sampling (taken back in and out), which tends to overstate the performance because of such "contamination." Other problems would be time consuming, computationally expensive, and it can be unstable on different samples.

# Question 3  
## Use only the training sets from question 1 and estimate an 8-fold cross-validation to estimate the R^2 of the full model. e., use cross-validation to train (on 7/8 of the training set) and evaluate (on 1/8 of the training set).  Calculate the mean R^2 from the 8-fold cross-validation and compare it with the R^2 from question 1. Please explain your observation.
```{r}
train_control <- trainControl(method="cv", number=8)

cross_model <- train(heart_attack ~., data=train,
               trControl = train_control,
               method = "lm")

print(cross_model)

# obtain R-squared
(cross_R <- cross_model$resample$Rsquared)
(mean_cross_R <- mean(cross_R))
```
From the results, R-squared is 0.8628722, which is slightly lower than the R-squared from Q1--0.8744722. I was expecting the cross validation would generate a higher R-squared. However, this can happen because the number of variables here are not that large, so there is a chance that the full model outperforms. Another possible reason could be that the data set is not a lot.


# Question 4  
## Explain Lasso regression and how does it work. List the pros and cons associated with using it
Lasso regression is a penalized regression method used to select variables; it is the sum of squared residuals plus lambda multiplied by the absolute value of slope. As lambda increases, Lasso shrinks betas accordingly (make predictions "less sensitive" to training data set). With the k-fold method, we fit the Lasso path for each fold and obtain deviance from the left out bucket. We then choose the best lambda and use it to refit the model.

One of the pros with Lasso is that it can shrink the beta to 0, so meaningless variables can be excluded from models, making the final model simpler and easier to understand. Another pro is that Lasso can help reduce over-fitting, and it is better than some usual methods of variable selection like forward and backward. 
One of the cons with Lasso is that it has some bias (by shrinking betas), and it makes estimating standard errors more difficult. Another con is that Lasso may ignore variables that are not significant but interesting or important.

# Question 5  
## a.Use again the training sets from question 1 and Fit a Lasso regression to predict the heart attack probability. Use cross-validation to obtain lambda_min as well as lambda_1se. Explain the two resulting models. Which one would you choose?
```{r}
library(glmnet)

x <- data.matrix(train[, 1:16])
y <- train$heart_attack
lasso_cv_model <- cv.glmnet(x, y, alpha = 1, nfolds = 8, family = "gaussian", standardize = TRUE)
summary(lasso_cv_model)

# obtain lambda_min as well as lambda_1se
(lambda_min <- lasso_cv_model$lambda.min)
(lambda_1se <- lasso_cv_model$lambda.1se)
plot(lasso_cv_model)
```
lambda_min means the minimal OOS MSE; lambda_1se means less variables within 1 standard error of the minimal lambda. I would choose the lambda_1se since their MSE do not differ that much and lambda_1se has a simpler model (principle of parsimony).
```{r}
#best model with lambda 1se
lasso_model2 <- glmnet(x, y,alpha=1, lambda = lambda_1se)

#use the best model to predict test dataset
test_x <- data.matrix(test[,1:16])
test_y <- test[,17]
test_y_pred2 <- predict(lasso_model2, s=lambda_1se, newx = test_x)

#obtain OOS R^2
lasso_dev <- sum((test_y - test_y_pred2)^2)
lasso_null_dev <- sum((test_y - mean(test_y_pred2))^2)

(lasso_OOS_R <- 1-(lasso_dev/lasso_null_dev))
```

## b. Compare model outputs from questions one, three, and five
```{r}
Q1 <- train_model_glm$coefficients
Q3 <- cross_model$finalModel$coefficients
Q5 <- coef(lasso_cv_model, select = "1se")

OOS_outputs <- cbind(OOS_R, mean_cross_R, lasso_OOS_R)
colnames(OOS_outputs) <- c("Q1:OOS_R", "Q2:OOS_R cross", "Q5:OOS_R lasso")
OOS_outputs

outputs <- cbind(Q1, Q3, Q5)
outputs
```
Based on the outputs of coefficients and OOS R-squared, the R-squared of 3 questions are all pretty close to each other. The full model (Q1) has the highest OOS R-squared. Again, this can happen because the number of variables here are not that large, so there is a chance that the full model outperforms. The lasso regression (Q5) got rid of 11 variables and has a slightly higher R-squared than cross validation without penalty (Q3), which had 17 variables. 

# Question 6
## What is AIC, and how is it calculated? When to use AICc (corrected AIC)?
AIC is a information criteria that evaluates how well a model fits a given data set in situations, or the approximate distance between a model and “truth”(lecture note). AIC is calculated by deviance+2*degree of freedom. When df or sample size is large, AICc should be used because AIC tends to overfit.
